{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple demo to process the compiled dataset\n",
    "# after the whole process, the files in original_compiled_data can be transformed to the files in data4training_model\n",
    "# we have already prepared the processed files for data4training_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128.433573916656 -86.1689137467893 -1.1658416418422064 -0.461670602840447\n87.93870313788229 -90.13426667492 -1.228943063125428 -1.0785883625221\n207.197249044782 -95.0502059205476 -1.3513368723624046 -0.543827700211486\n213\n82.9982087530354 -101.379902323728 -8.991664263881768 -4.3698299642188\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# the address to read files in original_compiled_data folder in the \"data\" folder\n",
    "prefix='./data/original_compiled_data/'\n",
    "\n",
    "# chemical feature information of involved drugs (including ECFP and molecular graphs)\n",
    "in_file=open(prefix+'drugcomb_alldruginfo_dict.pickle','rb')\n",
    "drugcomb_alldruginfo_dict=pickle.load(in_file)\n",
    "in_file.close()\n",
    "\n",
    "# therapeutic synergy score (TE) samples\n",
    "drugcomb=pd.read_csv(prefix+'drugcomb.csv')\n",
    "\n",
    "# adverse effect (AE) samples\n",
    "twosides=pd.read_csv(prefix+'twosides.csv')\n",
    "\n",
    "# drug-target interaction (DTI) samples\n",
    "drug_target=pd.read_csv(prefix+'drug_target_inter.csv')\n",
    "\n",
    "# protein-protein interaction (PPI) samples\n",
    "target_target=pd.read_csv(prefix+'target_target_inter.csv')\n",
    "\n",
    "drugcomb=drugcomb.rename(columns={'drug_row':'drug1','drug_col':'drug2'})\n",
    "twosides=twosides[['drug1','drug2','Polypharmacy Side Effect','Side Effect Name','drug1_lower','drug2_lower','unified_name']]\n",
    "\n",
    "# check synergy scores with None value\n",
    "# zip：None\n",
    "drugcomb['synergy_zip']=drugcomb['synergy_zip'].astype('float')\n",
    "print(drugcomb['synergy_zip'].max(),drugcomb['synergy_zip'].min(),drugcomb['synergy_zip'].mean(),drugcomb['synergy_zip'].median())\n",
    "\n",
    "# hsa: None\n",
    "drugcomb['synergy_hsa']=drugcomb['synergy_hsa'].astype('float')\n",
    "print(drugcomb['synergy_hsa'].max(),drugcomb['synergy_hsa'].min(),drugcomb['synergy_hsa'].mean(),drugcomb['synergy_hsa'].median())\n",
    "\n",
    "# bliss: None\n",
    "drugcomb['synergy_bliss']=drugcomb['synergy_bliss'].astype('float')\n",
    "print(drugcomb['synergy_bliss'].max(),drugcomb['synergy_bliss'].min(),drugcomb['synergy_bliss'].mean(),drugcomb['synergy_bliss'].median())\n",
    "\n",
    "# loewe: 4\n",
    "temp=drugcomb[drugcomb['synergy_loewe']!='\\\\N']\n",
    "temp_drugset=set.union(set(temp['drug1']),set(temp['drug2']))\n",
    "# remove these 4 samples\n",
    "print(len(temp_drugset))\n",
    "drugcomb=drugcomb.drop(index=(drugcomb.loc[(drugcomb['synergy_loewe']=='\\\\N')].index))\n",
    "drugcomb['synergy_loewe']=drugcomb['synergy_loewe'].astype('float')\n",
    "print(drugcomb['synergy_loewe'].max(),drugcomb['synergy_loewe'].min(),drugcomb['synergy_loewe'].mean(),drugcomb['synergy_loewe'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACCN2', 'GPR19', 'ABP1', 'ALPPL2', 'CA13', 'ACCN1'}\n{'gpr19', 'ca13', 'alppl2', 'abp1', 'accn2', 'accn1'}\n825 825 12796 12796\nEmpty DataFrame\nColumns: [0]\nIndex: []\n"
     ]
    }
   ],
   "source": [
    "# 处理完synergy score数据以后，首先需要给所有药物靶点定一个顺序，以生成邻接矩阵\n",
    "drugset=list(set.union(set(drugcomb['drug1_lower']),set(drugcomb['drug2_lower'])))\n",
    "# 给drugset使用sort确定一个固定顺序\n",
    "drugset.sort()\n",
    "drug2id_dict={drug: i for i,drug in enumerate(drugset)}\n",
    "\n",
    "# 再生成target的顺序\n",
    "drug_target=drug_target.rename(columns={'target':'gene symbol'})\n",
    "drug_target=drug_target[['drug','drug_lower','gene symbol']]\n",
    "temp=set(drug_target['gene symbol'])\n",
    "\n",
    "# remove the wrong sample\n",
    "target_target=target_target[['gene1 symbol','gene2 symbol']]\n",
    "target_target.iloc[103608]['gene1 symbol']='WTIP'\n",
    "targetset=list(set.union(set(target_target['gene1 symbol']),set(target_target['gene2 symbol'])))\n",
    "# 给targetset使用sort确定一个固定顺序\n",
    "targetset.sort()\n",
    "\n",
    "# further check data quality\n",
    "temp_2=set([target.lower() for target in temp])\n",
    "targetset_2=set([target.lower() for target in targetset])\n",
    "print(temp-set(targetset))\n",
    "print(temp_2-targetset_2)\n",
    "print(len(temp),len(temp_2),len(targetset),len(targetset_2))\n",
    "temp_3=pd.DataFrame([target.lower() for target in targetset])\n",
    "print(temp_3.iloc[temp_3.duplicated()[temp_3.duplicated()==True].index])\n",
    "target_target[(target_target['gene1 symbol'].str.lower() == 'wtip') | (target_target['gene2 symbol'].str.lower() == 'wtip')]\n",
    "\n",
    "# 将drug_target中独有的target也并入targetset\n",
    "targetset=list(set.union(set(targetset),temp))\n",
    "# 重新固定顺序\n",
    "targetset.sort()\n",
    "target2id_dict={target: i for i,target in enumerate(targetset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2837 2764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2764 2764\n"
     ]
    }
   ],
   "source": [
    "print(len(set(drugcomb['unified_name'])),len(set(twosides['unified_name'])))\n",
    "# 将drugcomb样本缩减至2764个，以与twosides对应\n",
    "twosides_pair_set=set(twosides['unified_name'])\n",
    "drugcomb_pair_set=set(drugcomb['unified_name'])\n",
    "inter_pair_set=list(set.intersection(twosides_pair_set,drugcomb_pair_set))\n",
    "# 将inter_pair_set顺序固定，这就可以减少一定的随机程度\n",
    "inter_pair_set.sort()\n",
    "\n",
    "# 将te数据集中所包含的总pair数量变为2764\n",
    "drugcomb_reduced=[]\n",
    "for row in np.array(drugcomb):\n",
    "    if(row[-1] in inter_pair_set):\n",
    "        drugcomb_reduced.append(row)\n",
    "# 代替之前读取的drugcomb\n",
    "drugcomb=pd.DataFrame(drugcomb_reduced,columns=drugcomb.columns)\n",
    "print(len(set(drugcomb['unified_name'])),len(set(twosides['unified_name'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7089 18392\n2694\n"
     ]
    }
   ],
   "source": [
    "# 现在开始基于上面的数据重新生成drugcomb和twosides，以及根据这两个数据集生成新的drug-target和target-target数据\n",
    "# 同时也会生成新的drugset和targetset\n",
    "\n",
    "# 开始尝试削减cell line数量\n",
    "# 先基于所有的cell line生成一个顺序字典\n",
    "cellline_set=list(set(drugcomb['cell_line_name']))\n",
    "# 固定set顺序\n",
    "cellline_set.sort()\n",
    "\n",
    "cellline_num_list=[]\n",
    "for cellline in cellline_set:\n",
    "    num=drugcomb[drugcomb['cell_line_name']==cellline].shape[0]\n",
    "    cellline_num_list.append([cellline,num])\n",
    "cellline_num_list=pd.DataFrame(cellline_num_list)\n",
    "# cellline_num_list是一个包含每个cellline所对应药物对数的list\n",
    "cellline_num_list = cellline_num_list.sort_values([1],ascending=(False)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7089 18392\n2694\n"
     ]
    }
   ],
   "source": [
    "# 根据20个cell line去得到对应的drug_comb样本\n",
    "# obtain the samples which have the same number as the description in the manuscript\n",
    "selected_cellline_num=20\n",
    "drugcomb_sorted=[]\n",
    "for _,cellline in cellline_num_list[:selected_cellline_num].iterrows():\n",
    "    drugcomb_sorted.append(drugcomb[drugcomb['cell_line_name']==cellline[0]])\n",
    "drugcomb_sorted=pd.concat(drugcomb_sorted,axis=0)\n",
    "drugpair_sorted=set(drugcomb_sorted['unified_name'])\n",
    "print(drugcomb_sorted.shape[0],drugcomb.shape[0])\n",
    "print(len(drugpair_sorted))\n",
    "# print(cellline_num_list[:selected_cellline_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当选择cell line总数设置为20时，大概drugcomb总样本数缩减至7000，约为本来的1/3\n",
    "# twosides原总样本数大概为19000, 而twosides样本数据基本未发生变化: 19317 to 18832\n",
    "# 同时，总drug pair数下降至2694，药物总数下降至159\n",
    "drugcomb_sorted=drugcomb_sorted.reset_index(drop=True)\n",
    "twosides_sorted=[]\n",
    "for _, row in twosides.iterrows():\n",
    "    if row['unified_name'] in drugpair_sorted:\n",
    "        twosides_sorted.append(row)\n",
    "twosides_sorted=pd.DataFrame(twosides_sorted,columns=twosides.columns).reset_index(drop=True)\n",
    "\n",
    "print(len(set.union(set(drugcomb_sorted['drug1_lower']),set(drugcomb_sorted['drug2_lower']))))\n",
    "print(len(set.union(set(twosides_sorted['drug1_lower']),set(twosides_sorted['drug2_lower']))))\n",
    "print(len(set(twosides_sorted['unified_name'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3920, 3) (100552, 2)\n"
     ]
    }
   ],
   "source": [
    "# 基于新生成的drugcomb和twosides去构建新的drugset和targetset\n",
    "# 同时需要处理drug-target以及target-target数据\n",
    "drugset=list(set.union(set(drugcomb_sorted['drug1_lower']),set(drugcomb_sorted['drug2_lower'])))\n",
    "# 用于给drugset使用sort确定一个固定顺序\n",
    "drugset.sort()\n",
    "drug2id_dict={drug: i for i,drug in enumerate(drugset)}\n",
    "\n",
    "drug_target_sorted=[]\n",
    "for row in np.array(drug_target):\n",
    "    if row[1] in drugset:\n",
    "        drug_target_sorted.append(row)\n",
    "drug_target_sorted=pd.DataFrame(drug_target_sorted,columns=drug_target.columns)\n",
    "\n",
    "targetset=set(drug_target_sorted['gene symbol'])\n",
    "target_target_sorted=[]\n",
    "for row in np.array(target_target):\n",
    "    # 根据drug_target筛选出对应的target_target\n",
    "    if ((row[0] in targetset) or (row[1] in targetset)):\n",
    "        target_target_sorted.append(row)\n",
    "target_target_sorted=pd.DataFrame(target_target_sorted,columns=target_target.columns)\n",
    "\n",
    "targetset=list(set.union(targetset, set.union(set(target_target_sorted['gene1 symbol']),set(target_target_sorted['gene2 symbol']))))\n",
    "targetset.sort()\n",
    "target2id_dict={target: i for i,target in enumerate(targetset)}\n",
    "\n",
    "print(drug_target_sorted.shape, target_target_sorted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2694.0 2694.0\ntotal unified name number in twosides: 2694\n[] []\n"
     ]
    }
   ],
   "source": [
    "# 根据drug编号生成药物之间的邻接矩阵\n",
    "# 非常重要：\n",
    "# 因为可能需要使药物-药物对样本加倍，以使A-B,B-A预测结果相同，但在此处仅按照数据集中顺序进行\n",
    "# 为防止数据泄露，仅考虑在训练时对样本数量进行加倍（也就是对除了药物-药物对之外的数据进行对角线对称操作）\n",
    "\n",
    "# 在HNEMA中，drug-drug-interaction和target-target-interaction矩阵在对角线位置的值为0（但两者都是对角线对称矩阵A=A.T）\n",
    "duplicated_te=[]\n",
    "drug_te_drug_matrix=np.zeros((len(drug2id_dict),len(drug2id_dict)))\n",
    "for _,row in drugcomb_sorted.iterrows():\n",
    "    # row\n",
    "    temp1=drug2id_dict[row['drug1_lower']]\n",
    "    # col\n",
    "    temp2=drug2id_dict[row['drug2_lower']]\n",
    "    # 检查是否有A-B/B-A情况\n",
    "    if(drug_te_drug_matrix[temp2,temp1]==1):\n",
    "        duplicated_te.append(row)\n",
    "    drug_te_drug_matrix[temp1,temp2]=1\n",
    "\n",
    "# 从这里发现，其实在两个数据库(drugcomb/twosides)中原始的drug name的大小写也是不同的，所以要用drug_lower\n",
    "duplicated_se=[]\n",
    "drug_se_drug_matrix=np.zeros((len(drug2id_dict),len(drug2id_dict)))\n",
    "for _,row in twosides_sorted.iterrows():\n",
    "    # row\n",
    "    temp1=drug2id_dict[row['drug1_lower']]\n",
    "    # col\n",
    "    temp2=drug2id_dict[row['drug2_lower']]\n",
    "    # 检查是否有A-B/B-A情况\n",
    "    if(drug_se_drug_matrix[temp2,temp1]==1):\n",
    "        duplicated_se.append(row)\n",
    "    drug_se_drug_matrix[temp1,temp2]=1\n",
    "\n",
    "# te样本是多于se的\n",
    "print(drug_se_drug_matrix.sum(),drug_te_drug_matrix.sum())\n",
    "print('total unified name number in twosides:',len(set(drugcomb_sorted['unified_name'])))\n",
    "# 无A-B/B-A情况\n",
    "print(duplicated_te,duplicated_se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pre-processing DD (TE relationship) meta-paths\n",
    "\n",
    "# A p-value, or probability value, is a number describing how likely it is that your data would have occurred by random chance (i.e. that the null hypothesis is true).\n",
    "# A p-value less than 0.05 (typically ≤ 0.05) is statistically significant. It indicates strong evidence against the null hypothesis, as there is less than a 5% probability the null is correct (and the results are random). Therefore, we reject the null hypothesis, and accept the alternative hypothesis.\n",
    "\n",
    "# PPF of the standard normal distribution for the probability 1-a = 0.95\n",
    "# scipy.stats.norm.ppf(0.95)=1.64, https://educationalresearchtechniques.com/2018/09/24/z-scores-and-inferential-stats-in-python/\n",
    "duplicated_te_new=[]\n",
    "synergy_score_qualified=[]\n",
    "drug_te_drug_matrix_new=np.zeros((len(drug2id_dict),len(drug2id_dict)))\n",
    "used_synergy_score='synergy_loewe'\n",
    "mean=np.mean(drugcomb_sorted[used_synergy_score])\n",
    "std=np.std(drugcomb_sorted[used_synergy_score])\n",
    "for _,row in drugcomb_sorted.iterrows():\n",
    "    # row\n",
    "    temp1=drug2id_dict[row['drug1_lower']]\n",
    "    # col\n",
    "    temp2=drug2id_dict[row['drug2_lower']]\n",
    "    z_score=(float(row[used_synergy_score]) - mean) / std\n",
    "    # 检查是否有A-B/B-A情况\n",
    "    if(drug_te_drug_matrix_new[temp2,temp1]==1):\n",
    "        duplicated_te_new.append(row)\n",
    "    if z_score>=1.64:\n",
    "        drug_te_drug_matrix_new[temp1,temp2]=1\n",
    "        synergy_score_qualified.append(float(row[used_synergy_score]))\n",
    "print(duplicated_te_new)\n",
    "# 这样相当于阈值变为了20.95\n",
    "print(len(synergy_score_qualified),np.max(synergy_score_qualified),np.min(synergy_score_qualified),np.mean(synergy_score_qualified))\n",
    "# 大概原来百分之十的药物对存在te边（193/2694）\n",
    "\n",
    "print(drug_te_drug_matrix_new.sum())\n",
    "# 此时的drug_te_drug_matrix_new是不对称的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成完drug-te-drug matrix以后再生成target-target interaction（生成对称矩阵）\n",
    "target_target_matrix=np.zeros((len(target2id_dict),len(target2id_dict)))\n",
    "for row in np.array(target_target_sorted):\n",
    "    # row\n",
    "    temp1=target2id_dict[row[0]]\n",
    "    # col\n",
    "    temp2=target2id_dict[row[1]]\n",
    "    target_target_matrix[temp1,temp2]=1\n",
    "    target_target_matrix[temp2,temp1]=1\n",
    "    \n",
    "# 再生成drug-target矩阵(drug用小写dict转换，gene用大写dict转换)\n",
    "drug_target_matrix=np.zeros((len(drug2id_dict),len(target2id_dict)))\n",
    "for row in np.array(drug_target_sorted):\n",
    "    # drug\n",
    "    temp1=drug2id_dict[row[1]]\n",
    "    # target\n",
    "    temp2=target2id_dict[row[2]]\n",
    "    drug_target_matrix[temp1,temp2]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此时共得到两个drug-drug（不对称）,一个drug-target（不对称），一个target-target（对称）matrices\n",
    "\n",
    "# 接下来处理drug SMILES子图数据\n",
    "drugcomb_alldruginfo_dict_lower={}\n",
    "for key in drugcomb_alldruginfo_dict.keys():\n",
    "    drugcomb_alldruginfo_dict_lower[key.lower()]=drugcomb_alldruginfo_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7089, 29), (18832, 7), (3920, 3), (100552, 2), (159, 12575))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of every type of samples\n",
    "drugcomb_sorted.shape, twosides_sorted.shape, drug_target_sorted.shape, target_target_sorted.shape, drug_target_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2694, 2694)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of drug pairs\n",
    "len(set(drugcomb_sorted['unified_name'])), len(set(twosides_sorted['unified_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the root for storing the files generated by the following code for model training (data4training_model folder in the \"data\" folder)\n",
    "prefix_sorted='./data/data4training_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将上述矩阵切换为稀疏矩阵\n",
    "from scipy import sparse\n",
    "# 这些矩阵的顺序都是有drug2id以及target2id的dict决定的\n",
    "drug_se_drug_coomatrix=sparse.coo_matrix(drug_se_drug_matrix)\n",
    "drug_te_drug_coomatrix=sparse.coo_matrix(drug_te_drug_matrix)\n",
    "drug_target_coomatrix=sparse.coo_matrix(drug_target_matrix)\n",
    "target_target_coomatrix=sparse.coo_matrix(target_target_matrix)\n",
    "\n",
    "# 存储新的te矩阵\n",
    "drug_te_drug_coomatrix_new=sparse.coo_matrix(drug_te_drug_matrix_new)\n",
    "# sparse.save_npz(prefix_sorted+'drug_te_drug_coomatrix_new.npz',drug_te_drug_coomatrix_new)\n",
    "# sparse.save_npz(prefix_sorted+'drug_se_drug_coomatrix.npz',drug_se_drug_coomatrix)\n",
    "# sparse.save_npz(prefix_sorted+'drug_te_drug_coomatrix.npz',drug_te_drug_coomatrix)\n",
    "# sparse.save_npz(prefix_sorted+'drug_target_coomatrix.npz',drug_target_coomatrix)\n",
    "# sparse.save_npz(prefix_sorted+'target_target_coomatrix.npz',target_target_coomatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drugcomb_sorted.to_csv(prefix_sorted+'drugcomb.csv',index=False)\n",
    "# twosides_sorted.to_csv(prefix_sorted+'twosides.csv',index=False)\n",
    "# drug_target_sorted.to_csv(prefix_sorted+'drug_target.csv',index=False)\n",
    "# target_target_sorted.to_csv(prefix_sorted+'target_target.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(prefix_sorted+'drug2id_dict.pickle','wb') as out_file:\n",
    "    pickle.dump(drug2id_dict,out_file)\n",
    "with open(prefix_sorted+'target2id_dict.pickle','wb') as out_file:\n",
    "    pickle.dump(target2id_dict,out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for generating molecular graphs for the variant HNE-GIN\n",
    "\n",
    "id2drug_dict={value: key for key,value in drug2id_dict.items()}\n",
    "drug_graph_edges=[]\n",
    "drug_graph_properties=[]\n",
    "# 储存node feature\n",
    "drug_graph_nodes=[]\n",
    "drugid2morgan={}\n",
    "for drugid in id2drug_dict.keys():\n",
    "    # drugid是当前drug id\n",
    "    druginfo=drugcomb_alldruginfo_dict_lower[id2drug_dict[drugid]]\n",
    "    drugid2morgan[drugid]=druginfo['morgan_bit']\n",
    "    counter=-1\n",
    "    # 得到矩阵的某一行\n",
    "    for row in druginfo['adjacent_matrix']:\n",
    "        counter+=1\n",
    "        # 循环矩阵某一行的非零元素的所在位置\n",
    "        for dst in row.nonzero()[0]:\n",
    "            drug_graph_edges.append([drugid,counter,dst])\n",
    "    # 再加入自环\n",
    "    temp=druginfo['adjacent_matrix'].shape[0]\n",
    "    self_loop=[[drugid,i,i] for i in range(temp)]\n",
    "    drug_graph_edges.extend(self_loop)\n",
    "    # 其实在properties中，graph_id就是label（也就是药物标签）\n",
    "    drug_graph_properties.append([drugid,drugid,temp])\n",
    "\n",
    "    # 直接保留原本原子的原子数\n",
    "    for atom in druginfo['atom_num']:\n",
    "        drug_graph_nodes.append([drugid,atom])\n",
    "\n",
    "drug_graph_edges=pd.DataFrame(drug_graph_edges,columns=['graph_id','src','dst'])\n",
    "drug_graph_properties=pd.DataFrame(drug_graph_properties,columns=['graph_id','label','num_nodes'])\n",
    "drug_graph_nodes=pd.DataFrame(drug_graph_nodes,columns=['graph_id','atom_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计一下出现的原子个数集合\n",
    "atom_set=list(set(drug_graph_nodes['atom_num']))\n",
    "atom_set.sort()\n",
    "id_transform={atom: i for i,atom in enumerate(atom_set)}\n",
    "print(id_transform)\n",
    "\n",
    "# a small test\n",
    "atom_embedding=torch.nn.Embedding(len(atom_set),64)\n",
    "atom_rel_list=[id_transform[i] for i in atom_set]\n",
    "print(atom_rel_list)\n",
    "atom_embedding=atom_embedding(torch.LongTensor(atom_rel_list))\n",
    "print(atom_embedding.shape)\n",
    "print(torch.LongTensor(atom_rel_list).size())\n",
    "\n",
    "# 存储atomnum2id_dict,用于在模型中生成原子的one-hot特征\n",
    "with open(prefix_sorted+'atomnum2id_dict.pickle','wb') as out_file:\n",
    "    pickle.dump(id_transform,out_file)\n",
    "\n",
    "drug_graph_edges.to_csv(prefix_sorted+'drug_graph_edges.csv',index=0)\n",
    "drug_graph_properties.to_csv(prefix_sorted+'drug_graph_properties.csv',index=0)\n",
    "drug_graph_nodes.to_csv(prefix_sorted+'drug_graph_nodes.csv',index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************************************************\n",
    "# start to generate the data which is suitable for HNEMA\n",
    "# ******************************************************\n",
    "\n",
    "# fix the random seed\n",
    "import random\n",
    "random_seed = 1012\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2694.0, 2694.0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 此时te和se中drug_pair数已经相同\n",
    "drug_te_drug_matrix.sum(), drug_se_drug_matrix.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drug_target\n",
    "drug_target_drugid=list(map(lambda n: drug2id_dict[n], list(drug_target_sorted['drug_lower'])))\n",
    "drug_target_targetid=list(map(lambda n: target2id_dict[n], list(drug_target_sorted['gene symbol'])))\n",
    "drug_target_sorted['drugid']=drug_target_drugid\n",
    "drug_target_sorted['targetid']=drug_target_targetid\n",
    "\n",
    "# target_target\n",
    "target_target_targetid1=list(map(lambda n: target2id_dict[n], list(target_target_sorted['gene1 symbol'])))\n",
    "target_target_targetid2=list(map(lambda n: target2id_dict[n], list(target_target_sorted['gene2 symbol'])))\n",
    "target_target_sorted['targetid1']=target_target_targetid1\n",
    "target_target_sorted['targetid2']=target_target_targetid2\n",
    "\n",
    "# drug_se_drug\n",
    "# ***********************************************************************************\n",
    "# in this place, we also generated another type of meta-path, DD (AE/SE relationship)\n",
    "# but we do not use it in current version of HNEMA\n",
    "# ***********************************************************************************\n",
    "drug_se_drug_drugid1=list(map(lambda n: drug2id_dict[n], list(twosides_sorted['drug1_lower'])))\n",
    "drug_se_drug_drugid2=list(map(lambda n: drug2id_dict[n], list(twosides_sorted['drug2_lower'])))\n",
    "twosides_sorted['drugid1']=drug_se_drug_drugid1\n",
    "twosides_sorted['drugid2']=drug_se_drug_drugid2\n",
    "\n",
    "# drug_te_drug\n",
    "drug_te_drug_drugid1=list(map(lambda n: drug2id_dict[n], list(drugcomb_sorted['drug1_lower'])))\n",
    "drug_te_drug_drugid2=list(map(lambda n: drug2id_dict[n], list(drugcomb_sorted['drug2_lower'])))\n",
    "drugcomb_sorted['drugid1']=drug_te_drug_drugid1\n",
    "drugcomb_sorted['drugid2']=drug_te_drug_drugid2\n",
    "\n",
    "# 这里drug_se_drug和drug_te_drug是分开的,在选择完作为训练集的drug-drug pair序号以后，将两个dataframe合并成一个dataframe(用drug序号表示)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "twosides_pair_set=set(twosides_sorted['unified_name'])\n",
    "drugcomb_pair_set=set(drugcomb_sorted['unified_name'])\n",
    "inter_pair_set=list(set.intersection(twosides_pair_set,drugcomb_pair_set))\n",
    "inter_pair_set.sort()\n",
    "\n",
    "folds=10\n",
    "val_fold=8\n",
    "test_fold=9\n",
    "\n",
    "def data_split_for_training(folds,val_fold,test_fold):\n",
    "    if val_fold<test_fold:\n",
    "        del_list=[val_fold,test_fold]\n",
    "    else:\n",
    "        del_list=[test_fold,val_fold]\n",
    "\n",
    "    prng = np.random.RandomState(random_seed)\n",
    "    allindex = prng.permutation(len(inter_pair_set))\n",
    "    pos_inter_fold = np.array_split(allindex, folds)\n",
    "\n",
    "    val_pos_sample = pos_inter_fold[val_fold]\n",
    "    test_pos_sample = pos_inter_fold[test_fold]\n",
    "    train_pos_sample = deepcopy(pos_inter_fold)\n",
    "    train_pos_sample.pop(del_list[0])\n",
    "    train_pos_sample.pop(del_list[1]-1)\n",
    "    train_pos_sample = np.concatenate(train_pos_sample)\n",
    "    return np.array(sorted(train_pos_sample)), np.array(sorted(val_pos_sample)), np.array(sorted(test_pos_sample))\n",
    "\n",
    "train_idx, val_idx, test_idx = data_split_for_training(folds,val_fold,test_fold)   \n",
    "    \n",
    "drugpair2id_dict={drugpair: i for i,drugpair in enumerate(inter_pair_set)}\n",
    "\n",
    "# keys的顺序就是对应从0到最后一个，由于在生成训练集测试集时，index已经进行了打乱，在这里不需要进一步打乱\n",
    "all_drugpair_name=np.array(list(drugpair2id_dict.keys()))\n",
    "train_drugpair_name=all_drugpair_name[train_idx]\n",
    "val_drugpair_name=all_drugpair_name[val_idx]\n",
    "test_drugpair_name=all_drugpair_name[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2694, 2694)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check again\n",
    "len(inter_pair_set), train_idx.shape[0]+val_idx.shape[0]+test_idx.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据划分的出的pair确定相关的样本\n",
    "drugcomb_train,drugcomb_val,drugcomb_test=[],[],[]\n",
    "for _,row in drugcomb_sorted.iterrows():\n",
    "    if(row['unified_name'] in set(train_drugpair_name)):\n",
    "        drugcomb_train.append(row)\n",
    "    if(row['unified_name'] in set(val_drugpair_name)):\n",
    "        drugcomb_val.append(row)\n",
    "    if(row['unified_name'] in set(test_drugpair_name)):\n",
    "        drugcomb_test.append(row)\n",
    "\n",
    "drugcomb_train=pd.DataFrame(drugcomb_train,columns=drugcomb_sorted.columns).reset_index(drop=True)\n",
    "drugcomb_val=pd.DataFrame(drugcomb_val,columns=drugcomb_sorted.columns).reset_index(drop=True)\n",
    "drugcomb_test=pd.DataFrame(drugcomb_test,columns=drugcomb_sorted.columns).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugcomb_train=drugcomb_train.sort_values(['drugid1','drugid2'],ascending=(True,True)).reset_index(drop=True)\n",
    "drugcomb_val=drugcomb_val.sort_values(['drugid1','drugid2'],ascending=(True,True)).reset_index(drop=True)\n",
    "drugcomb_test=drugcomb_test.sort_values(['drugid1','drugid2'],ascending=(True,True)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "twosides_train,twosides_val,twosides_test=[],[],[]\n",
    "for _,row in twosides_sorted.iterrows():\n",
    "    if(row['unified_name'] in set(train_drugpair_name)):\n",
    "        twosides_train.append(row)\n",
    "    if(row['unified_name'] in set(val_drugpair_name)):\n",
    "        twosides_val.append(row)\n",
    "    if(row['unified_name'] in set(test_drugpair_name)):\n",
    "        twosides_test.append(row)\n",
    "\n",
    "twosides_train=pd.DataFrame(twosides_train,columns=twosides_sorted.columns).reset_index(drop=True)\n",
    "twosides_val=pd.DataFrame(twosides_val,columns=twosides_sorted.columns).reset_index(drop=True)\n",
    "twosides_test=pd.DataFrame(twosides_test,columns=twosides_sorted.columns).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "twosides_train=twosides_train.sort_values(['drugid1','drugid2'],ascending=(True,True)).reset_index(drop=True)\n",
    "twosides_val=twosides_val.sort_values(['drugid1','drugid2'],ascending=(True,True)).reset_index(drop=True)\n",
    "twosides_test=twosides_test.sort_values(['drugid1','drugid2'],ascending=(True,True)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start to build the adjacency matrix for heterogeneous network embedding\n",
    "cellline_set=list(set(drugcomb_sorted['cell_line_name']))\n",
    "cellline_set.sort()\n",
    "\n",
    "cellline2id_dict={cellline: i for i,cellline in enumerate(cellline_set)}\n",
    "# with open(prefix+'cellline2id_dict.pickle','wb') as out_file:\n",
    "    # pickle.dump(cellline2id_dict,out_file)\n",
    "    \n",
    "# the node order: 0 for drug, 1 for target, 2 for cell line\n",
    "num_drug=len(drug2id_dict)\n",
    "num_target=len(target2id_dict)\n",
    "num_cellline=len(cellline2id_dict)\n",
    "dim=num_drug+num_target+num_cellline\n",
    "\n",
    "# give a type mask to every node in the dataset (because there are four node types)\n",
    "type_mask=np.zeros((dim),dtype=int)\n",
    "type_mask[num_drug:num_drug+num_target]=1\n",
    "type_mask[num_drug+num_target:]=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n0.0\n5388.0 2694.0 5388.0 2694.0\nnum_drug: 159 num_target: 12575\n"
     ]
    }
   ],
   "source": [
    "# 这一步需要生成邻接矩阵去存储异构网络中的所有关系，并且所有关系都是对称的\n",
    "# 也就是说接下来生成的adjM是一个对角线对称矩阵，但其中验证集和测试集中的drug-drug pair需要去掉\n",
    "# 而在之前所有drug-drug pair都不是对称的，以防止数据泄露，此时已经在训练集测试集分开\n",
    "# 在这里可以考虑将drug-drug pair矩阵变为对称矩阵（参考DeepSynergy）\n",
    "# 也就是说即使这里double了样本，也不会重复出现数据泄露，因为训练集和测试集基于不同的drug-pair进行了完全的区分\n",
    "\n",
    "# test（证明当te和se矩阵对称时，两者是完全相同的，所以此时两者可以共享一个邻接矩阵区域，也就是drug-inter-drug区域）\n",
    "drug_se_drug_matrix_symmetric=np.zeros(drug_se_drug_matrix.shape)\n",
    "counter=-1\n",
    "for row in np.array(drug_se_drug_matrix):\n",
    "    counter+=1\n",
    "    for col in row.nonzero()[0]:\n",
    "        drug_se_drug_matrix_symmetric[col,counter]=1\n",
    "drug_se_drug_matrix_symmetric+=np.array(drug_se_drug_matrix)\n",
    "print((drug_se_drug_matrix_symmetric!=drug_se_drug_matrix_symmetric.T).sum())\n",
    "\n",
    "drug_te_drug_matrix_symmetric=np.zeros(drug_te_drug_matrix.shape)\n",
    "counter=-1\n",
    "for row in np.array(drug_te_drug_matrix):\n",
    "    counter+=1\n",
    "    for col in row.nonzero()[0]:\n",
    "        drug_te_drug_matrix_symmetric[col,counter]=1\n",
    "drug_te_drug_matrix_symmetric+=np.array(drug_te_drug_matrix)\n",
    "# print((drug_se_drug_matrix_symmetric!=drug_se_drug_matrix_symmetric.T).sum())\n",
    "# print((drug_te_drug_matrix_symmetric!=drug_te_drug_matrix_symmetric.T).sum())\n",
    "print((drug_se_drug_matrix_symmetric-drug_te_drug_matrix_symmetric).sum())\n",
    "\n",
    "print(drug_te_drug_matrix_symmetric.sum(),drug_te_drug_matrix.sum(),drug_se_drug_matrix_symmetric.sum(),drug_se_drug_matrix.sum())\n",
    "print('num_drug:',num_drug,'num_target:',num_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(211, 6)\n"
     ]
    }
   ],
   "source": [
    "# 证明对称的se和te是一样之后，生成用于计算对称drug-drug矩阵的dataframe(此时的drug_drug并不是对称的)\n",
    "# drug_drug来自于drugcomb_train,可用于生成drug_se_drug的关系\n",
    "drug_drug_sorted=drugcomb_train[['drug1','drug2','drug1_lower','drug2_lower','drugid1','drugid2']]\n",
    "\n",
    "# 但是drug-drug dataframe只用于生成drug-se-drug关系，还需要生成一个用于生成drug-te-drug关系(基于drug_drug)\n",
    "# 但根据上面分析，这里只需生成训练集中的te关系即可\n",
    "drug_te_drug_sorted=[]\n",
    "for _,row in drug_drug_sorted.iterrows():\n",
    "    temp1,temp2=row['drugid1'],row['drugid2']\n",
    "    if ((drug_te_drug_matrix_new[temp1,temp2]==1) or (drug_te_drug_matrix_new[temp2,temp1]==1)):\n",
    "        drug_te_drug_sorted.append(row)\n",
    "        \n",
    "# 共有211个样本（based on drug-drug-cell line pairs）> 193个总样本数（in drug_te_drug_matrix_new）\n",
    "# 因为drugcomb_train中针对每个药物对可能会有多个cell line,但是对于d-te-d/d-se-d,只需保留一个综合结果即可\n",
    "# 所以要在后面生成d-d metapath样本时，要进行去重操作\n",
    "drug_te_drug_sorted=pd.DataFrame(drug_te_drug_sorted,columns=['drug1','drug2','drug1_lower','drug2_lower','drugid1','drugid2']).reset_index(drop=True)\n",
    "print(drug_te_drug_sorted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个邻接矩阵是中心对称的（例如将原来drug-target数据，变为了D-T,T-D两部分在adjM中）\n",
    "adjM=np.zeros((dim,dim),dtype=int)\n",
    "\n",
    "# store drug-target (0-1)\n",
    "drug_target_sorted=drug_target_sorted.sort_values(['drugid','targetid'],ascending=(True,True)).reset_index(drop=True)\n",
    "for _,row in drug_target_sorted.iterrows():\n",
    "    uid=row['drugid']\n",
    "    aid=num_drug+row['targetid']\n",
    "    adjM[uid, aid] = 1\n",
    "    adjM[aid, uid] = 1\n",
    "    \n",
    "# store target-target (1-1)\n",
    "target_target_sorted=target_target_sorted.sort_values(['targetid1','targetid2'],ascending=(True,True)).reset_index(drop=True)\n",
    "for _,row in target_target_sorted.iterrows():\n",
    "    uid=num_drug+row['targetid1']\n",
    "    aid=num_drug+row['targetid2']\n",
    "    adjM[uid,aid]=1\n",
    "    adjM[aid,uid]=1\n",
    "    \n",
    "adjM_te = deepcopy(adjM)\n",
    "# store drug-se-drug (0-se-0)\n",
    "# 存储的是训练集中的对称drug-drug pair (基于所有的te pair或所有的se pair)\n",
    "for _,row in drug_drug_sorted.iterrows():\n",
    "    uid=row['drugid1']\n",
    "    aid=row['drugid2']\n",
    "    adjM[uid,aid]=1\n",
    "    adjM[aid,uid]=1\n",
    "    \n",
    "# store drug-te-drug (0-te-0)\n",
    "# 存储的是训练集中的对称drug-drug pair (基于大于阈值的te pair)\n",
    "for _,row in drug_te_drug_sorted.iterrows():\n",
    "    uid=row['drugid1']\n",
    "    aid=row['drugid2']\n",
    "    adjM_te[uid,aid]=1\n",
    "    adjM_te[aid,uid]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********************************************************************************************************\n",
    "# obtain dicts storing the neighbors of different nodes with diferent types (based on different metapaths)\n",
    "# ********************************************************************************************************\n",
    "\n",
    "# metapath/edge types for drug embedding:\n",
    "# 1.drug-target-drug (drug-target)\n",
    "# 2.drug-target-target-drug (drug-target,target-target)\n",
    "# 3.drug-target-target-target-drug (drug-target,target-target)\n",
    "# 4.drug-se-drug (drug-drug) (finally it is not used)\n",
    "# 5.drug-te-drug (drug-drug)\n",
    "\n",
    "# indices that the dict stores are all based on the relative index instead of absolute index\n",
    "# 也是就list中的关系都是用相对标签来储存\n",
    "\n",
    "# 取得的就是每个drug所对应target的关系(可通过绘制adjM的对应区域来证明)\n",
    "drug_target_list = {i: adjM[i, num_drug:].nonzero()[0] for i in range(num_drug)}\n",
    "# 取得的就是每个target所对应drug的关系\n",
    "target_drug_list = {\n",
    "    i: adjM[i+num_drug, :num_drug].nonzero()[0] for i in range(num_target)}\n",
    "# 取得的就是每个target所对应的target的关系(由于基于adjM,所以里面的结果是对称的)\n",
    "target_target_list = {\n",
    "    i: adjM[i+num_drug, num_drug:].nonzero()[0] for i in range(num_target)}\n",
    "# 取得的就是每个drug所对应的drug的关系(基于se或者所有的te)\n",
    "drug_drug_list = {\n",
    "    i: adjM[i, :num_drug].nonzero()[0] for i in range(num_drug)}\n",
    "# 取得的就是每个drug所对应的drug的关系(超过阈值的te)\n",
    "drug_te_drug_list = {\n",
    "    i: adjM_te[i, :num_drug].nonzero()[0] for i in range(num_drug)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drug_target_drug.shape: (43906, 3)\n"
     ]
    }
   ],
   "source": [
    "# generate different metapath-based datasets (based on absolute indices)\n",
    "\n",
    "# metapath index uses absoluate index\n",
    "# drug-target-drug (0-1-0)\n",
    "drug_target_drug=[]\n",
    "# 相比drug_target_list,target_drug_list会稀疏很多，因为只有小部分target与drug直接相连\n",
    "for target, drug_list in target_drug_list.items():\n",
    "    # the order of drug1 and drug2 should not be important\n",
    "    drug_target_drug.extend([(drug1, target, drug2)\n",
    "                            for drug1 in drug_list for drug2 in drug_list])\n",
    "drug_target_drug = np.array(drug_target_drug)\n",
    "# transform all the node indices to absolute indices\n",
    "drug_target_drug[:, 1] += num_drug\n",
    "sorted_index = sorted(list(range(len(drug_target_drug))),\n",
    "                      key=lambda i: drug_target_drug[i, [0, 2, 1]].tolist())\n",
    "# sort the drug_target_drug according to the priority order: drug1->drug2->target\n",
    "# 也就是以两边的drug需要来进行排序\n",
    "# 此时所包含的是绝对标签\n",
    "drug_target_drug = drug_target_drug[sorted_index]\n",
    "print('drug_target_drug.shape:',drug_target_drug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_inter_target.shape: (201104, 2)\n"
     ]
    }
   ],
   "source": [
    "# drug-target-target-drug (0-1-1-0)\n",
    "# 先获得target-inter-target (1-1)\n",
    "# drug1-target1-target2-drug2和drug2-target2-target1-drug1都是有意义的，并且这个关系只有在将target-target变成对称的情况下才能生成\n",
    "# 否则只有target1-target2或target2-target1，上述两种情况分别用于生成drug2的embedding和drug1的embedding\n",
    "target_inter_target=target_target_sorted[['targetid1','targetid2']].to_numpy(dtype=np.int32)\n",
    "target_inter_target=np.concatenate([target_inter_target,target_inter_target[:,[1,0]]],axis=0)\n",
    "target_inter_target=[tuple(row) for row in target_inter_target]\n",
    "target_inter_target=np.unique(target_inter_target,axis=0)\n",
    "# 在这里不需要变为绝对标签，因为下一步搜索所需的target_drug_list基于相对标签\n",
    "# target_inter_target += num_drug\n",
    "sorted_index = sorted(list(range(len(target_inter_target))),\n",
    "                      key=lambda i: target_inter_target[i].tolist())\n",
    "target_inter_target = target_inter_target[sorted_index]\n",
    "print('target_inter_target.shape:',target_inter_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drug_target_target_drug.shape: (1620274, 4)\n"
     ]
    }
   ],
   "source": [
    "# 再获得0-1-1-0\n",
    "drug_target_target_drug=[]\n",
    "for target1,target2 in target_inter_target:\n",
    "     drug_target_target_drug.extend([(drug1, target1, target2, drug2) for drug1 in target_drug_list[target1] for drug2 in target_drug_list[target2]])\n",
    "drug_target_target_drug=np.array(drug_target_target_drug)\n",
    "#将相对标签转换为绝对标签\n",
    "drug_target_target_drug[:, [1, 2]] += num_drug\n",
    "sorted_index = sorted(list(range(len(drug_target_target_drug))), key=lambda i : drug_target_target_drug[i, [0, 3, 1, 2]].tolist())\n",
    "drug_target_target_drug = drug_target_target_drug[sorted_index]\n",
    "print('drug_target_target_drug.shape:',drug_target_target_drug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drug-target-target-target-drug (0-1-1-1-0)\n",
    "# 先获得target-target-target (1-1-1)\n",
    "target_target_target=[]\n",
    "for target,target_list in target_target_list.items():\n",
    "    target_target_target.extend([(target1,target,target2) for target1 in target_list for target2 in target_list])\n",
    "target_target_target=np.array(target_target_target)\n",
    "# 目前没有将相对坐标转化成绝对坐标\n",
    "# 因为在target_drug_list中使用的是相对坐标\n",
    "sorted_index=sorted(list(range(len(target_target_target))),key=lambda i: target_target_target[i,[0,2,1]].tolist())\n",
    "target_target_target=target_target_target[sorted_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_target_target.shape: (39462620, 3)\n"
     ]
    }
   ],
   "source": [
    "print('target_target_target.shape:',target_target_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drug_target_target_target_drug.shape: (7116096, 5)\n"
     ]
    }
   ],
   "source": [
    "drug_target_target_target_drug=[]\n",
    "# this sampling ratio could be further adjusted according to the computational resources you have\n",
    "dtttd_ratio=0.3\n",
    "for target1,target,target2 in target_target_target:\n",
    "    # target_drug_list使用相对坐标，目前target1,target,target2都是相对坐标\n",
    "    if(len(target_drug_list[target1])==0 or len(target_drug_list[target])==0):\n",
    "        continue\n",
    "    candidate_drug1_list=np.random.choice(len(target_drug_list[target1]),\n",
    "                                          int(dtttd_ratio*len(target_drug_list[target1])),replace=False)\n",
    "    candidate_drug1_list=target_drug_list[target1][candidate_drug1_list]\n",
    "\n",
    "    candidate_drug2_list=np.random.choice(len(target_drug_list[target2]),\n",
    "                                         int(dtttd_ratio*len(target_drug_list[target2])),replace=False)\n",
    "    candidate_drug2_list=target_drug_list[target2][candidate_drug2_list]\n",
    "    \n",
    "    drug_target_target_target_drug.extend([(drug1,target1,target,target2,drug2) for drug1 in candidate_drug1_list for drug2 in candidate_drug2_list])\n",
    "\n",
    "drug_target_target_target_drug=np.array(drug_target_target_target_drug)\n",
    "#将相对标签转换为绝对标签\n",
    "drug_target_target_target_drug[:, [1, 2, 3]] += num_drug\n",
    "sorted_index=sorted(list(range(len(drug_target_target_target_drug))),key=lambda i:drug_target_target_target_drug[i,[0,4,1,2,3]].tolist())\n",
    "drug_target_target_target_drug=drug_target_target_target_drug[sorted_index]\n",
    "print('drug_target_target_target_drug.shape:',drug_target_target_target_drug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drug_inter1_drug.shape: (294, 2)\n"
     ]
    }
   ],
   "source": [
    "# 然后处理drug-drug数据，在这里要注意只使用训练集中drug-drug pair作为样本\n",
    "# 因为这些metapath用于训练模型,所以所有的drug-drug pair都是对称的\n",
    "# 同0-1-1-0，有了drug1-drug2和drug2-drug1才能同时为drug1和drug2生成embedding\n",
    "\n",
    "# 这里处理的是经过阈值筛选的te training子图\n",
    "# metapath index for therapeutic effect\n",
    "drug_inter1_drug=drug_te_drug_sorted[['drugid1','drugid2']].to_numpy(dtype=np.int32)\n",
    "drug_inter1_drug=np.concatenate([drug_inter1_drug,drug_inter1_drug[:,[1,0]]],axis=0)\n",
    "drug_inter1_drug=[tuple(row) for row in drug_inter1_drug]\n",
    "drug_inter1_drug=np.unique(drug_inter1_drug,axis=0)\n",
    "\n",
    "sorted_index = sorted(list(range(len(drug_inter1_drug))),\n",
    "                      key=lambda i: drug_inter1_drug[i].tolist())\n",
    "drug_inter1_drug = drug_inter1_drug[sorted_index]\n",
    "print('drug_inter1_drug.shape:',drug_inter1_drug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drug_inter2_drug.shape: (4312, 2)\n"
     ]
    }
   ],
   "source": [
    "# metapath index for side effect (it is not used in current HNEMA)\n",
    "drug_inter2_drug=drug_drug_sorted[['drugid1','drugid2']].to_numpy(dtype=np.int32)\n",
    "drug_inter2_drug=np.concatenate([drug_inter2_drug,drug_inter2_drug[:,[1,0]]],axis=0)\n",
    "drug_inter2_drug=[tuple(row) for row in drug_inter2_drug]\n",
    "drug_inter2_drug=np.unique(drug_inter2_drug,axis=0)\n",
    "\n",
    "sorted_index = sorted(list(range(len(drug_inter2_drug))),\n",
    "                      key=lambda i: drug_inter2_drug[i].tolist())\n",
    "drug_inter2_drug = drug_inter2_drug[sorted_index]\n",
    "print('drug_inter2_drug.shape:',drug_inter2_drug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在训练集某个batch中，上述两个集合中与要预测的pair重复的pair会被去掉（正负方向都会去掉）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the saving process of generated meta-path instances\n",
    "import pathlib\n",
    "\n",
    "# 先基于所有的cell line生成一个顺序字典\n",
    "cellline_set=list(set(drugcomb_sorted['cell_line_name']))\n",
    "cellline_set.sort()\n",
    "cellline2id_dict={cellline: i for i,cellline in enumerate(cellline_set)}\n",
    "with open(prefix_sorted+'cellline2id_dict.pickle','wb') as out_file:\n",
    "    pickle.dump(cellline2id_dict,out_file)\n",
    "\n",
    "drugpair2id_dict={drugpair: i for i,drugpair in enumerate(inter_pair_set)}\n",
    "\n",
    "involved_metapaths=[\n",
    "    [(0,1,0),(0,1,1,0),(0,1,1,1,0),(0,'te',0),(0,'se',0)]\n",
    "]\n",
    "\n",
    "for i in range(len(involved_metapaths)):\n",
    "    pathlib.Path(prefix_sorted+'{}'.format(i)).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "metapath_indices_mapping = {\n",
    "    (0, 1, 0): drug_target_drug,\n",
    "    (0, 1, 1, 0): drug_target_target_drug,\n",
    "    (0, 1, 1, 1, 0): drug_target_target_target_drug,\n",
    "    (0, 'te', 0): drug_inter1_drug,\n",
    "    (0, 'se', 0): drug_inter2_drug,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all data（按照metapath_indices_mapping逻辑储存即可）\n",
    "# 实际上没有用上protein target标签，因为这里只生成了基于drug的metapath\n",
    "target_idx_lists = [np.arange(num_drug), np.arange(num_target)]\n",
    "offset_list = [0, num_drug]\n",
    "for i, metapaths in enumerate(involved_metapaths):\n",
    "    for metapath in metapaths:\n",
    "        # get corresponding metapath nodes\n",
    "        edge_metapath_idx_array = metapath_indices_mapping[metapath]\n",
    "        \n",
    "        # store the metapath node as pickle file\n",
    "        with open(prefix_sorted+'{}/'.format(i)+'-'.join(map(str, metapath))+'_idx.pickle', 'wb') as out_file:\n",
    "            target_metapaths_mapping={}\n",
    "            left=0\n",
    "            right=0\n",
    "            for target_idx in target_idx_lists[i]:\n",
    "                # target_idx refers a specific drug or target specified by an index (在这里也就是每个药物的id)\n",
    "                # the aim is to locate the last position of a metapath of a node in edge_metapath_idx_array\n",
    "                # edge_metapath_idx_array is ordered by the first node in a metapath\n",
    "                while right < len(edge_metapath_idx_array) and edge_metapath_idx_array[right, 0] == target_idx + offset_list[i]:\n",
    "                    right += 1\n",
    "                # first select all metapath choices using the first node, then put the first node into the last position of the metapath choice as the target node\n",
    "                # edge_metapath_idx_array[left:right, ::-1]: reverse every metapath choice\n",
    "                # and then store them to 'target_metapaths_mapping'\n",
    "                # 也就是说edge_metapath_idx_array是按照第0个元素进行排序的，所以可通过left和right获得每个药物节点所对应的所有元路径\n",
    "                # 在GCN中，最后一个节点是中心节点，也就是target节点，所以需要将每个metapath颠倒一下再放入\n",
    "                target_metapaths_mapping[target_idx] = edge_metapath_idx_array[left:right, ::-1]\n",
    "                # move to the next drug/target node\n",
    "                left = right\n",
    "            # write dict after iterations\n",
    "            pickle.dump(target_metapaths_mapping, out_file)\n",
    "        \n",
    "        # store the corresponding source and target node of metapath choice as adjlist file\n",
    "        # 再存储所有metapath中中心节点的邻居节点\n",
    "        with open(prefix_sorted + '{}/'.format(i) + '-'.join(map(str, metapath)) + '.adjlist', 'w') as out_file:\n",
    "            left = 0\n",
    "            right = 0\n",
    "            for target_idx in target_idx_lists[i]:\n",
    "                while right < len(edge_metapath_idx_array) and edge_metapath_idx_array[right, 0] == target_idx + offset_list[i]:\n",
    "                    right += 1\n",
    "                # adjlist is based on relative index\n",
    "                neighbors = edge_metapath_idx_array[left:right, -1] - \\\n",
    "                    offset_list[i]\n",
    "                neighbors = list(map(str, neighbors))\n",
    "                if(len(neighbors) > 0):\n",
    "                    # write line in each iteration\n",
    "                    # 格式： central node + neighboring nodes(可能包括他们自身)\n",
    "                    # 例子： 9 9 9 15 28 69 154 184 332 343 449 477 543 544 549 567 570 574 654 669 669 683 (第一个9是central node)\n",
    "                    out_file.write('{} '.format(target_idx) +\n",
    "                                   ' '.join(neighbors) + '\\n')\n",
    "                else:\n",
    "                    out_file.write('{}\\n'.format(target_idx))\n",
    "                left = right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy import sparse\n",
    "\n",
    "# 此时验证集和测试集中的drug-drug pair还未加倍（也就是还没有生成drug pairs with the opposite drug order）\n",
    "# drugcomb_train.to_csv(prefix_sorted+'drugcomb_train.csv',index=0)\n",
    "# drugcomb_val.to_csv(prefix_sorted+'drugcomb_val.csv',index=0)\n",
    "# drugcomb_test.to_csv(prefix_sorted+'drugcomb_test.csv',index=0)\n",
    "\n",
    "# twosides_train.to_csv(prefix_sorted+'twosides_train.csv',index=0)\n",
    "# twosides_val.to_csv(prefix_sorted+'twosides_val.csv',index=0)\n",
    "# twosides_test.to_csv(prefix_sorted+'twosides_test.csv',index=0)\n",
    "\n",
    "# store the heterogeneous adjacent matrix using a sparse npz file (which stores the model training data)\n",
    "scipy.sparse.save_npz(prefix_sorted+'adjM.npz',\n",
    "                      scipy.sparse.csr_matrix(adjM))\n",
    "# store the node type mask of all nodes\n",
    "np.save(prefix_sorted+'node_types.npy', type_mask)\n",
    "\n",
    "# 在HNEMA中，最终需要重新生成一个完整的drug-target的numpy列表并储存，因为drug-target在操作过程中被替换为了训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2156 269 269\n"
     ]
    }
   ],
   "source": [
    "# start to generate each drug-drug-cell line pair with AE and TE labels/values\n",
    "\n",
    "# 根据划分的出的pair确定相关的样本\n",
    "drugcomb_train,drugcomb_val,drugcomb_test=[],[],[]\n",
    "\n",
    "# drugcomb在缩减为2764条样本之后再没有被修改过\n",
    "for _,row in drugcomb_sorted.iterrows():\n",
    "    if(row['unified_name'] in set(train_drugpair_name)):\n",
    "        drugcomb_train.append(row)\n",
    "    if(row['unified_name'] in set(val_drugpair_name)):\n",
    "        drugcomb_val.append(row)\n",
    "    if(row['unified_name'] in set(test_drugpair_name)):\n",
    "        drugcomb_test.append(row)\n",
    "\n",
    "drugcomb_train=pd.DataFrame(drugcomb_train,columns=drugcomb_sorted.columns).reset_index(drop=True)\n",
    "drugcomb_val=pd.DataFrame(drugcomb_val,columns=drugcomb_sorted.columns).reset_index(drop=True)\n",
    "drugcomb_test=pd.DataFrame(drugcomb_test,columns=drugcomb_sorted.columns).reset_index(drop=True)\n",
    "\n",
    "print(train_drugpair_name.shape[0],val_drugpair_name.shape[0],test_drugpair_name.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将训练集的样本量加倍\n",
    "drugcomb_train=drugcomb_train[['drugid1','drugid2','cell_line_name','S_mean','synergy_zip','synergy_loewe','synergy_hsa','synergy_bliss']]\n",
    "drugcomb_train_=drugcomb_train[['drugid2','drugid1','cell_line_name','S_mean','synergy_zip','synergy_loewe','synergy_hsa','synergy_bliss']]\n",
    "drugcomb_train=pd.DataFrame(np.concatenate([np.array(drugcomb_train),np.array(drugcomb_train_)],axis=0),columns=drugcomb_train.columns)\n",
    "drugcomb_train=drugcomb_train.sort_values(['drugid1', 'drugid2'],ascending=(True,True)).reset_index(drop=True)\n",
    "\n",
    "# 在训练集中使样本数量翻倍，在验证集和测试集中保持不变\n",
    "# 然后在模型的一个batch中，将样本翻倍并评估，而不是在这里\n",
    "# 因为此时一个batch中每个样本都会有两个顺序，将测试集generator大小减半即可\n",
    "drugcomb_val=drugcomb_val[['drugid1','drugid2','cell_line_name','S_mean','synergy_zip','synergy_loewe','synergy_hsa','synergy_bliss']]\n",
    "# drugcomb_val_=drugcomb_val[['drugid2','drugid1','cell_line_name','S_mean','synergy_zip','synergy_loewe','synergy_hsa','synergy_bliss']]\n",
    "# drugcomb_val=pd.DataFrame(np.concatenate([np.array(drugcomb_val),np.array(drugcomb_val_)],axis=0),columns=drugcomb_val.columns)\n",
    "drugcomb_val=drugcomb_val.sort_values(['drugid1', 'drugid2'],ascending=(True,True)).reset_index(drop=True)\n",
    "\n",
    "drugcomb_test=drugcomb_test[['drugid1','drugid2','cell_line_name','S_mean','synergy_zip','synergy_loewe','synergy_hsa','synergy_bliss']]\n",
    "# drugcomb_test_=drugcomb_test[['drugid2','drugid1','cell_line_name','S_mean','synergy_zip','synergy_loewe','synergy_hsa','synergy_bliss']]\n",
    "# drugcomb_test=pd.DataFrame(np.concatenate([np.array(drugcomb_test),np.array(drugcomb_test_)],axis=0),columns=drugcomb_test.columns)\n",
    "drugcomb_test=drugcomb_test.sort_values(['drugid1', 'drugid2'],ascending=(True,True)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11254, 8), (812, 8), (650, 8))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drugcomb_train.shape,drugcomb_val.shape,drugcomb_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将所用样本和标签重新生成一次\n",
    "twosides_train,twosides_val,twosides_test=[],[],[]\n",
    "for _,row in twosides_sorted.iterrows():\n",
    "    if(row['unified_name'] in set(train_drugpair_name)):\n",
    "        twosides_train.append(row)\n",
    "    if(row['unified_name'] in set(val_drugpair_name)):\n",
    "        twosides_val.append(row)\n",
    "    if(row['unified_name'] in set(test_drugpair_name)):\n",
    "        twosides_test.append(row)\n",
    "\n",
    "twosides_train=pd.DataFrame(twosides_train,columns=twosides_sorted.columns).reset_index(drop=True)\n",
    "twosides_val=pd.DataFrame(twosides_val,columns=twosides_sorted.columns).reset_index(drop=True)\n",
    "twosides_test=pd.DataFrame(twosides_test,columns=twosides_sorted.columns).reset_index(drop=True)\n",
    "\n",
    "twosides_train=twosides_train[['drugid1','drugid2','Polypharmacy Side Effect','Side Effect Name']]\n",
    "twosides_train_=twosides_train[['drugid2','drugid1','Polypharmacy Side Effect','Side Effect Name']]\n",
    "twosides_train=pd.DataFrame(np.concatenate([np.array(twosides_train),np.array(twosides_train_)],axis=0),columns=twosides_train.columns)\n",
    "twosides_train=twosides_train.sort_values(['drugid1', 'drugid2'],ascending=(True,True)).reset_index(drop=True)\n",
    "\n",
    "twosides_val=twosides_val[['drugid1','drugid2','Polypharmacy Side Effect','Side Effect Name']]\n",
    "# twosides_val_=twosides_val[['drugid2','drugid1','Polypharmacy Side Effect','Side Effect Name']]\n",
    "# twosides_val=pd.DataFrame(np.concatenate([np.array(twosides_val),np.array(twosides_val_)],axis=0),columns=twosides_val.columns)\n",
    "twosides_val=twosides_val.sort_values(['drugid1', 'drugid2'],ascending=(True,True)).reset_index(drop=True)\n",
    "\n",
    "twosides_test=twosides_test[['drugid1','drugid2','Polypharmacy Side Effect','Side Effect Name']]\n",
    "# twosides_test_=twosides_test[['drugid2','drugid1','Polypharmacy Side Effect','Side Effect Name']]\n",
    "# twosides_test=pd.DataFrame(np.concatenate([np.array(twosides_test),np.array(twosides_test_)],axis=0),columns=twosides_test.columns)\n",
    "twosides_test=twosides_test.sort_values(['drugid1', 'drugid2'],ascending=(True,True)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并drugcomb和twosides\n",
    "# 首先生成twosides的标签顺序\n",
    "se_symbolset=list(set(twosides_sorted['Polypharmacy Side Effect']))\n",
    "se_symbolset.sort()\n",
    "se_symbol2id_dict={se: i for i,se in enumerate(se_symbolset)}\n",
    "\n",
    "with open(prefix_sorted+'se_symbol2id_dict.pickle','wb') as out_file:\n",
    "    pickle.dump(se_symbol2id_dict,out_file)\n",
    "\n",
    "# start to create adverse effect labels based on drugcomb/TE samples (as TE prediction is our main task)\n",
    "\n",
    "# training\n",
    "train_se_label=np.zeros((drugcomb_train.shape[0],len(se_symbol2id_dict)))\n",
    "drugcomb_train_numpy=np.array(drugcomb_train)\n",
    "twosides_train_numpy=np.array(twosides_train)\n",
    "counter=-1\n",
    "counter_=0\n",
    "for row in np.array(drugcomb_train_numpy):\n",
    "    counter+=1\n",
    "    drugid1=row[0]\n",
    "    drugid2=row[1]\n",
    "    search=twosides_train_numpy[((twosides_train_numpy[:,0]==drugid1)&(twosides_train_numpy[:,1]==drugid2)) |\n",
    "                                ((twosides_train_numpy[:,1]==drugid1)&(twosides_train_numpy[:,0]==drugid2))]\n",
    "    search=[se_symbol2id_dict[i] for i in search[:,2]]\n",
    "    counter_+=len(search)\n",
    "    train_se_label[counter,search]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "val_se_label=np.zeros((drugcomb_val.shape[0],len(se_symbol2id_dict)))\n",
    "drugcomb_val_numpy=np.array(drugcomb_val)\n",
    "twosides_val_numpy=np.array(twosides_val)\n",
    "counter=-1\n",
    "for row in np.array(drugcomb_val_numpy):\n",
    "    counter+=1\n",
    "    drugid1=row[0]\n",
    "    drugid2=row[1]\n",
    "    search=twosides_val_numpy[((twosides_val_numpy[:,0]==drugid1)&(twosides_val_numpy[:,1]==drugid2)) |\n",
    "                            ((twosides_val_numpy[:,1]==drugid1)&(twosides_val_numpy[:,0]==drugid2))]\n",
    "    search=[se_symbol2id_dict[i] for i in search[:,2]]\n",
    "    val_se_label[counter,search]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_se_label=np.zeros((drugcomb_test.shape[0],len(se_symbol2id_dict)))\n",
    "drugcomb_test_numpy=np.array(drugcomb_test)\n",
    "twosides_test_numpy=np.array(twosides_test)\n",
    "counter=-1\n",
    "for row in np.array(drugcomb_test_numpy):\n",
    "    counter+=1\n",
    "    drugid1=row[0]\n",
    "    drugid2=row[1]\n",
    "    search=twosides_test_numpy[((twosides_test_numpy[:,0]==drugid1)&(twosides_test_numpy[:,1]==drugid2)) |\n",
    "                        ((twosides_test_numpy[:,1]==drugid1)&(twosides_test_numpy[:,0]==drugid2))]\n",
    "    search=[se_symbol2id_dict[i] for i in search[:,2]]\n",
    "    test_se_label[counter,search]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于drugcomb的样本建立te标签(顺序：S_mean，synergy_zip，synergy_loewe，synergy_hsa，synergy_bliss)\n",
    "train_te_label=np.array(drugcomb_train)[:,3:]\n",
    "val_te_label=np.array(drugcomb_val)[:,3:]\n",
    "test_te_label=np.array(drugcomb_test)[:,3:]\n",
    "\n",
    "# npy file can store one standard binary numpy matrix, while npz as a zip could store multiple npy(s)\n",
    "np.savez(prefix_sorted+'train_val_test_drug_drug_samples.npz',\n",
    "         train_drug_drug_samples=np.array(drugcomb_train,dtype=str)[:,:3],\n",
    "         val_drug_drug_samples=np.array(drugcomb_val,dtype=str)[:,:3],\n",
    "         test_drug_drug_samples=np.array(drugcomb_test,dtype=str)[:,:3])\n",
    "\n",
    "np.savez(prefix_sorted+'train_val_test_drug_drug_labels.npz',\n",
    "         train_te_labels=train_te_label.astype('float32'),\n",
    "         train_se_labels=train_se_label.astype('float32'),\n",
    "         val_te_labels=val_te_label.astype('float32'),\n",
    "         val_se_labels=val_se_label.astype('float32'),\n",
    "         test_te_labels=test_te_label.astype('float32'),\n",
    "         test_se_labels=test_se_label.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ECFP6 of each drug\n",
    "ECFP6_DNN=[]\n",
    "for key in drugid2morgan.keys():\n",
    "    ECFP6_DNN.append(drugid2morgan[key])\n",
    "ECFP6_DNN=np.array(ECFP6_DNN)\n",
    "\n",
    "ECFP6_DNN_coomatrix=sparse.coo_matrix(ECFP6_DNN)\n",
    "sparse.save_npz(prefix_sorted+'ECFP6_DNN_coomatrix.npz',ECFP6_DNN_coomatrix)\n",
    "print(ECFP6_DNN.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch19",
   "language": "python",
   "name": "pytorch19"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
